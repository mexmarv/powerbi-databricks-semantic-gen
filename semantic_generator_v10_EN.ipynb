{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_section",
      "metadata": {},
      "source": [
        "# Power BI Semantic Layer Generator > Notebook - Most DAX functions \n",
        "\n",
        "This notebook automates the creation of a comprehensive semantic layer in Databricks by extracting and transforming your Power BI dataset definitions. It serves as a bridge between Power BI's modeling capabilities and Databricks' scalable compute environment.\n",
        "\n",
        "## Key Features\n",
        "- **Automated View Generation**: Creates SQL views for all base tables from your Power BI dataset\n",
        "- **DAX to PySpark Translation**: Converts Power BI DAX measures into equivalent PySpark code\n",
        "  - Aggregate Functions:\n",
        "    - Basic (SUM, COUNT, AVERAGE, MIN, MAX)\n",
        "    - Statistical (VAR.P, STDEV.P)\n",
        "    - Distinct aggregations (DISTINCTCOUNT)\n",
        "  - Time Intelligence Functions:\n",
        "    - Period comparisons (DATEADD, SAMEPERIODLASTYEAR)\n",
        "    - Year-to-date (DATESYTD, DATESMTD, DATESQTD)\n",
        "    - Period navigation (PREVIOUSMONTH, PREVIOUSYEAR)\n",
        "  - Text Functions:\n",
        "    - String operations (CONCATENATE, UPPER, LOWER)\n",
        "    - Text manipulation (LEN, TRIM, SUBSTITUTE)\n",
        "  - Logical Functions:\n",
        "    - Conditional (IF, SWITCH)\n",
        "    - Boolean operations (AND, OR, NOT)\n",
        "  - Mathematical Functions:\n",
        "    - Basic math (ABS, ROUND, FLOOR, CEILING)\n",
        "    - Advanced calculations (POWER, SQRT)\n",
        "  - Filter Functions:\n",
        "    - Context modification (CALCULATE, FILTER)\n",
        "    - Table operations (ALL, ALLEXCEPT)\n",
        "  - Window Functions:\n",
        "    - Rankings (RANKX, TOPN)\n",
        "    - First/Last values (FIRSTNONBLANK, LASTNONBLANK)\n",
        "- **Relationship Handling**: \n",
        "  - Automated join view generation\n",
        "  - Preservation of Power BI relationship cardinality\n",
        "  - Support for multiple relationship paths\n",
        "- **Delta Lake Integration**: \n",
        "  - Optional materialization of views as Delta tables\n",
        "  - Performance optimization through materialized views\n",
        "  - Automatic refresh handling\n",
        "- **Documentation Generation**:\n",
        "  - Measure definitions and translations\n",
        "  - Relationship documentation\n",
        "  - Data lineage tracking\n",
        "\n",
        "## Prerequisites\n",
        "1. **Databricks Setup**:\n",
        "   - Workspace with appropriate permissions\n",
        "   - Ability to create tables and views\n",
        "   - PySpark environment configuration\n",
        "2. **Power BI Access**:\n",
        "   - Workspace admin privileges\n",
        "   - Dataset access permissions\n",
        "   - XMLA endpoint access\n",
        "3. **Azure AD Application**:\n",
        "   - Registered app with Power BI API permissions\n",
        "   - Client ID and secret\n",
        "   - API scope configuration\n",
        "\n",
        "## Configuration Guide\n",
        "1. **Azure AD Setup**:\n",
        "   - Register app in Azure Active Directory\n",
        "   - Grant Power BI API permissions\n",
        "   - Generate client secret\n",
        "2. **Power BI Setup**:\n",
        "   - Note workspace ID and dataset ID\n",
        "   - Ensure dataset is accessible\n",
        "   - Configure XMLA endpoints\n",
        "3. **Parameters**:\n",
        "   - Fill in credentials below\n",
        "   - Set materialization preference\n",
        "   - Configure refresh schedules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "client_id = \"YOUR_CLIENT_ID\"  # Azure AD application ID\n",
        "client_secret = \"YOUR_CLIENT_SECRET\"  # Azure AD client secret\n",
        "tenant_id = \"YOUR_TENANT_ID\"  # Azure AD tenant ID\n",
        "workspace_id = \"YOUR_WORKSPACE_ID\"  # Power BI workspace ID\n",
        "dataset_id = \"YOUR_DATASET_ID\"  # Power BI dataset ID\n",
        "semantic_model_name = \"Sales\"  # Name for generated semantic model\n",
        "materialize = True  # Set to True to create Delta tables\n",
        "\n",
        "# Install required dependencies\n",
        "%pip install msal requests nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auth_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authentication Setup\n",
        "import msal\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def get_access_token():\n",
        "    \"\"\"Get Power BI API access token using MSAL\"\"\"\n",
        "    app = msal.ConfidentialClientApplication(\n",
        "        client_id,\n",
        "        authority=f\"https://login.microsoftonline.com/{tenant_id}\",\n",
        "        client_credential=client_secret\n",
        "    )\n",
        "    \n",
        "    result = app.acquire_token_for_client(\n",
        "        scopes=[\"https://analysis.windows.net/powerbi/api/.default\"]\n",
        "    )\n",
        "    \n",
        "    if \"access_token\" not in result:\n",
        "        raise Exception(f\"Failed to get token: {result.get('error_description')}\")\n",
        "        \n",
        "    return result[\"access_token\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Power BI API Functions\n",
        "def get_dataset_info():\n",
        "    \"\"\"Retrieve dataset definition from Power BI API\"\"\"\n",
        "    token = get_access_token()\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    \n",
        "    url = f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"API call failed: {response.text}\")\n",
        "        \n",
        "    return response.json()\n",
        "\n",
        "def get_dataset_tables():\n",
        "    \"\"\"Extract tables from dataset\"\"\"\n",
        "    dataset = get_dataset_info()\n",
        "    return dataset.get(\"tables\", [])\n",
        "\n",
        "def get_dataset_relationships():\n",
        "    \"\"\"Extract relationships from dataset\"\"\"\n",
        "    dataset = get_dataset_info()\n",
        "    return dataset.get(\"relationships\", [])\n",
        "\n",
        "def get_dataset_measures():\n",
        "    \"\"\"Extract DAX measures from dataset\"\"\"\n",
        "    dataset = get_dataset_info()\n",
        "    measures = []\n",
        "    for table in dataset.get(\"tables\", []):\n",
        "        measures.extend(table.get(\"measures\", []))\n",
        "    return measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generator_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Semantic Layer Generation\n",
        "import nbformat as nbf\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def generate_view_definitions(tables):\n",
        "    \"\"\"Generate SQL view definitions for tables\"\"\"\n",
        "    view_cells = []\n",
        "    for table in tables:\n",
        "        view_name = table[\"name\"]\n",
        "        view_cells.extend([\n",
        "            nbf.v4.new_markdown_cell(f\"## Base View: {view_name}\"),\n",
        "            nbf.v4.new_code_cell(\n",
        "                f\"CREATE OR REPLACE VIEW semantic.{view_name} AS\\n\"\n",
        "                f\"SELECT * FROM raw.{view_name};\"\n",
        "            )\n",
        "        ])\n",
        "        if materialize:\n",
        "            view_cells.append(nbf.v4.new_code_cell(\n",
        "                f\"CREATE OR REPLACE TABLE semantic.{view_name}_materialized AS\\n\"\n",
        "                f\"SELECT * FROM semantic.{view_name};\"\n",
        "            ))\n",
        "    return view_cells\n",
        "\n",
        "def generate_relationships(relationships):\n",
        "    \"\"\"Generate joined views based on relationships\"\"\"\n",
        "    join_cells = []\n",
        "    for rel in relationships:\n",
        "        from_table = rel[\"fromTable\"]\n",
        "        to_table = rel[\"toTable\"]\n",
        "        from_column = rel[\"fromColumn\"]\n",
        "        to_column = rel[\"toColumn\"]\n",
        "        \n",
        "        view_name = f\"vw_{from_table}_{to_table}\"\n",
        "        join_cells.extend([\n",
        "            nbf.v4.new_markdown_cell(f\"## Joined View: {view_name}\"),\n",
        "            nbf.v4.new_code_cell(\n",
        "                f\"CREATE OR REPLACE VIEW semantic.{view_name} AS\\n\"\n",
        "                f\"SELECT f.*, d.*\\n\"\n",
        "                f\"FROM semantic.{from_table} f\\n\"\n",
        "                f\"LEFT JOIN semantic.{to_table} d ON f.{from_column} = d.{to_column};\"\n",
        "            )\n",
        "        ])\n",
        "    return join_cells\n",
        "\n",
        "def translate_dax_to_pyspark(measure):\n",
        "    \"\"\"Translate DAX measures to PySpark code\"\"\"\n",
        "    name = measure[\"name\"]\n",
        "    expression = measure[\"expression\"]\n",
        "    \n",
        "    # Aggregate Functions\n",
        "    agg_functions = {\n",
        "        'SUM': lambda col: f\"df.agg(F.sum('{col}')).alias('{name}')\",\n",
        "        'COUNT': lambda col: f\"df.agg(F.count('{col}')).alias('{name}')\",\n",
        "        'DISTINCTCOUNT': lambda col: f\"df.agg(F.countDistinct('{col}')).alias('{name}')\",\n",
        "        'MIN': lambda col: f\"df.agg(F.min('{col}')).alias('{name}')\",\n",
        "        'MAX': lambda col: f\"df.agg(F.max('{col}')).alias('{name}')\",\n",
        "        'AVERAGE': lambda col: f\"df.agg(F.avg('{col}')).alias('{name}')\",\n",
        "        'VAR.P': lambda col: f\"df.agg(F.var_pop('{col}')).alias('{name}')\",\n",
        "        'STDEV.P': lambda col: f\"df.agg(F.stddev_pop('{col}')).alias('{name}')\"\n",
        "    }\n",
        "\n",
        "    # Time Intelligence Functions\n",
        "    time_functions = {\n",
        "        'DATEADD': lambda date_col, num, interval: f\"df.withColumn('{name}', F.add_months(F.col('{date_col}'), {num}))\",\n",
        "        'DATESYTD': lambda date_col: f\"df.filter(F.col('{date_col}') <= F.last_day(F.current_date(), 'year'))\",\n",
        "        'DATESMTD': lambda date_col: f\"df.filter(F.col('{date_col}') <= F.last_day(F.current_date(), 'month'))\",\n",
        "        'DATESQTD': lambda date_col: f\"df.filter(F.col('{date_col}') <= F.last_day(F.current_date(), 'quarter'))\",\n",
        "        'SAMEPERIODLASTYEAR': lambda date_col: f\"df.withColumn('{name}', F.add_months(F.col('{date_col}'), -12))\",\n",
        "        'PREVIOUSMONTH': lambda date_col: f\"df.withColumn('{name}', F.add_months(F.col('{date_col}'), -1))\",\n",
        "        'PREVIOUSYEAR': lambda date_col: f\"df.withColumn('{name}', F.add_months(F.col('{date_col}'), -12))\"\n",
        "    }\n",
        "\n",
        "    # Text Functions\n",
        "    text_functions = {\n",
        "        'CONCATENATE': lambda *cols: f\"df.withColumn('{name}', F.concat({', '.join([f'F.col(\\\"{c}\\\")' for c in cols])})\",\n",
        "        'UPPER': lambda col: f\"df.withColumn('{name}', F.upper(F.col('{col}')))\",\n",
        "        'LOWER': lambda col: f\"df.withColumn('{name}', F.lower(F.col('{col}')))\",\n",
        "        'LEN': lambda col: f\"df.withColumn('{name}', F.length(F.col('{col}')))\",\n",
        "        'TRIM': lambda col: f\"df.withColumn('{name}', F.trim(F.col('{col}')))\",\n",
        "        'SUBSTITUTE': lambda text, old, new: f\"df.withColumn('{name}', F.regexp_replace(F.col('{text}'), '{old}', '{new}'))\"\n",
        "    }\n",
        "\n",
        "    # Logical Functions\n",
        "    logical_functions = {\n",
        "        'IF': lambda cond, true_val, false_val: f\"df.withColumn('{name}', F.when({cond}, {true_val}).otherwise({false_val}))\",\n",
        "        'SWITCH': lambda expr, *cases: f\"df.withColumn('{name}', F.case({', '.join([f'F.when({c[0]}, {c[1]})' for c in zip(cases[::2], cases[1::2])])}))\",\n",
        "        'AND': lambda *conds: f\"df.filter({' & '.join([f'({c})' for c in conds])})\",\n",
        "        'OR': lambda *conds: f\"df.filter({' | '.join([f'({c})' for c in conds])})\",\n",
        "        'NOT': lambda cond: f\"df.filter(~({cond}))\"\n",
        "    }\n",
        "\n",
        "    # Mathematical Functions\n",
        "    math_functions = {\n",
        "        'ABS': lambda col: f\"df.withColumn('{name}', F.abs(F.col('{col}')))\",\n",
        "        'ROUND': lambda col, decimals: f\"df.withColumn('{name}', F.round(F.col('{col}'), {decimals}))\",\n",
        "        'FLOOR': lambda col: f\"df.withColumn('{name}', F.floor(F.col('{col}')))\",\n",
        "        'CEILING': lambda col: f\"df.withColumn('{name}', F.ceil(F.col('{col}')))\",\n",
        "        'POWER': lambda col, power: f\"df.withColumn('{name}', F.pow(F.col('{col}'), {power}))\",\n",
        "        'SQRT': lambda col: f\"df.withColumn('{name}', F.sqrt(F.col('{col}')))\"\n",
        "    }\n",
        "\n",
        "    # Filter and Calculate Functions\n",
        "    filter_functions = {\n",
        "        'CALCULATE': lambda expr, *filters: f\"\"\"\n",
        "            df.filter({' & '.join([f'({f})' for f in filters])})\n",
        "            .agg({expr}).alias('{name}')\n",
        "        \"\"\",\n",
        "        'FILTER': lambda table, condition: f\"df.filter({condition})\",\n",
        "        'ALL': lambda table: f\"df.select('*')\",\n",
        "        'ALLEXCEPT': lambda table, *cols: f\"df.select({', '.join([f'F.col(\\\"{c}\\\")' for c in cols])})\"\n",
        "    }\n",
        "\n",
        "    # Window Functions\n",
        "    window_functions = {\n",
        "        'RANKX': lambda table, expr: f\"df.withColumn('{name}', F.rank().over(Window.orderBy(F.desc('{expr}'))))\",\n",
        "        'TOPN': lambda n, table, orderBy: f\"df.orderBy(F.desc('{orderBy}')).limit({n})\",\n",
        "        'FIRSTNONBLANK': lambda col, expr: f\"df.filter(F.col('{col}').isNotNull()).first()\",\n",
        "        'LASTNONBLANK': lambda col, expr: f\"df.filter(F.col('{col}').isNotNull()).orderBy(F.desc('{col}')).first()\"\n",
        "    }\n",
        "\n",
        "    # Parse expression and identify function\n",
        "    import re\n",
        "    func_match = re.match(r'(\\w+)\\((.*)\\)', expression)\n",
        "    if not func_match:\n",
        "        return \"# Could not parse DAX expression\"\n",
        "\n",
        "    func_name = func_match.group(1)\n",
        "    args = [arg.strip() for arg in func_match.group(2).split(',')]\n",
        "\n",
        "    # Find and execute appropriate translation\n",
        "    all_functions = {\n",
        "        **agg_functions,\n",
        "        **time_functions,\n",
        "        **text_functions,\n",
        "        **logical_functions,\n",
        "        **math_functions,\n",
        "        **filter_functions,\n",
        "        **window_functions\n",
        "    }\n",
        "\n",
        "    if func_name in all_functions:\n",
        "        try:\n",
        "            return all_functions[func_name](*args)\n",
        "        except Exception as e:\n",
        "            return f\"# Error translating {func_name}: {str(e)}\"\n",
        "    \n",
        "    return f\"# Translation not implemented for function: {func_name}\"\n",
        "\n",
        "def generate_measures(measures):\n",
        "    \"\"\"Generate PySpark code for DAX measures\"\"\"\n",
        "    measure_cells = []\n",
        "    for measure in measures:\n",
        "        measure_cells.extend([\n",
        "            nbf.v4.new_markdown_cell(\n",
        "                f\"### Measure: {measure['name']}\\n\"\n",
        "                f\"**DAX:** `{measure['expression']}`\"\n",
        "            ),\n",
        "            nbf.v4.new_code_cell(\n",
        "                \"from pyspark.sql import functions as F\\n\"\n",
        "                \"df = spark.table('semantic.FactSales')\\n\"\n",
        "                f\"{translate_dax_to_pyspark(measure)}\"\n",
        "            )\n",
        "        ])\n",
        "    return measure_cells\n",
        "\n",
        "def generate_semantic_notebook():\n",
        "    \"\"\"Generate complete semantic layer notebook\"\"\"\n",
        "    nb = nbf.v4.new_notebook()\n",
        "    \n",
        "    # Add documentation\n",
        "    nb.cells.append(nbf.v4.new_markdown_cell(\n",
        "        f\"# Generated Semantic Layer: {semantic_model_name}\\n\\n\"\n",
        "        \"This notebook contains the generated semantic layer based on your Power BI dataset.\"\n",
        "    ))\n",
        "    \n",
        "    # Generate components\n",
        "    tables = get_dataset_tables()\n",
        "    relationships = get_dataset_relationships()\n",
        "    measures = get_dataset_measures()\n",
        "    \n",
        "    nb.cells.extend(generate_view_definitions(tables))\n",
        "    nb.cells.extend(generate_relationships(relationships))\n",
        "    nb.cells.extend(generate_measures(measures))\n",
        "    \n",
        "    # Add materialization status\n",
        "    if materialize:\n",
        "        nb.cells.extend([\n",
        "            nbf.v4.new_markdown_cell(\"### âœ… Materialization Active\"),\n",
        "            nbf.v4.new_code_cell(\n",
        "                \"print('Delta tables have been created for improved performance.')\"\n",
        "            )\n",
        "        ])\n",
        "    \n",
        "    # Save notebook\n",
        "    output_path = f\"semantic_{semantic_model_name.lower()}.ipynb\"\n",
        "    nbf.write(nb, output_path)\n",
        "    print(f\"Generated semantic layer notebook: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute generation\n",
        "generate_semantic_notebook()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
