{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589bb716",
   "metadata": {},
   "source": [
    "# Power BI to Databricks Notebooks - Semantic Generator (JSON Import Mode)\n",
    "\n",
    "This notebook generates a Databricks-compatible semantic layer from a Power BI report's semantic model. It supports:\n",
    "\n",
    "- Extraction of tables, columns, measures, calculated columns, hierarchies, and relationships\n",
    "- DAX to Databricks SQL translation for most aggregation, logical, time intelligence, and text functions\n",
    "- PySpark fallback for unsupported or complex DAX logic\n",
    "- Materialization of metrics and dimensions as Databricks SQL views (if enabled)\n",
    "- Output as a Databricks-importable notebook\n",
    "\n",
    "**How to export the semantic model from Power BI Desktop:**\n",
    "1. Open your report in Power BI Desktop.\n",
    "2. Go to `File` → `Export` → `Power BI template` (`.pbit`).\n",
    "3. Rename the exported `.pbit` file to `.zip` and extract it.\n",
    "4. Inside the extracted folder, locate the `DataModelSchema` or `DataModel` JSON file (usually named `DataModelSchema` or `DataModel`).\n",
    "5. Place this JSON file in your working directory and set the `PBI_JSON_PATH` variable in the configuration cell below to its filename.\n",
    "\n",
    "The notebook will read this JSON file and generate Databricks SQL and PySpark code for your semantic layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN\n",
    "PBI_JSON_PATH = 'DataModelSchema.json'  # Path to exported Power BI semantic model JSON\n",
    "materialize = True  # Set to True to create SQL views in Databricks\n",
    "catalog_name = 'main'\n",
    "schema_name = 'semantic'\n",
    "output_notebook = 'generated_semantic_notebook.ipynb'  # Output Databricks notebook name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77beaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load semantic model from exported Power BI JSON\n",
    "with open(PBI_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    pbi_model = json.load(f)\n",
    "\n",
    "model = defaultdict(list)\n",
    "\n",
    "# Extract tables, columns, measures, calculated columns, relationships, hierarchies\n",
    "for table in pbi_model.get('model', {}).get('tables', []):\n",
    "    table_name = table['name']\n",
    "    for col in table.get('columns', []):\n",
    "        model['columns'].append({'TABLE_NAME': table_name, 'COLUMN_NAME': col['name']})\n",
    "        if col.get('isCalculated', False):\n",
    "            model['calculated_columns'].append({'TABLE': table_name, 'NAME': col['name'], 'EXPRESSION': col.get('expression', '')})\n",
    "    for measure in table.get('measures', []):\n",
    "        model['measures'].append({'MEASURE_NAME': measure['name'], 'EXPRESSION': measure['expression'], 'MEASUREGROUP_NAME': table_name})\n",
    "    for hierarchy in table.get('hierarchies', []):\n",
    "        model['hierarchies'].append({'TABLE_NAME': table_name, 'HIERARCHY_NAME': hierarchy['name'], 'LEVELS': [lvl['name'] for lvl in hierarchy.get('levels', [])]})\n",
    "for rel in pbi_model.get('model', {}).get('relationships', []):\n",
    "    model['relationships'].append(rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat.v4 import new_notebook, new_code_cell\n",
    "from nbformat import write\n",
    "import re\n",
    "\n",
    "def translate_dax_advanced(dax_expression, fallback_to_pyspark=False):\n",
    "    \"\"\"\n",
    "    Translates DAX expressions to SQL/PySpark equivalent. Fallbacks to PySpark if SQL is not possible.\n",
    "    \"\"\"\n",
    "    # Remove any comments\n",
    "    dax_expression = re.sub(r'//.*?$', '', dax_expression, flags=re.MULTILINE)\n",
    "\n",
    "    # Common DAX function mappings to SQL/PySpark\n",
    "    translations = {\n",
    "        # Aggregation functions\n",
    "        r'SUM\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'SUM(\\1)',\n",
    "        r'AVERAGE\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'AVG(\\1)',\n",
    "        r'COUNT\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'COUNT(\\1)',\n",
    "        r'DISTINCTCOUNT\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'COUNT(DISTINCT \\1)',\n",
    "        r'MIN\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'MIN(\\1)',\n",
    "        r'MAX\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'MAX(\\1)',\n",
    "        # Logical functions\n",
    "        r'IF\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'CASE WHEN \\1 THEN \\2 ELSE \\3 END',\n",
    "        r'AND\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'(\\1 AND \\2)',\n",
    "        r'OR\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'(\\1 OR \\2)',\n",
    "        r'NOT\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'NOT (\\1)',\n",
    "        # Date functions\n",
    "        r'YEAR\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'EXTRACT(YEAR FROM \\1)',\n",
    "        r'MONTH\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'EXTRACT(MONTH FROM \\1)',\n",
    "        r'DAY\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'EXTRACT(DAY FROM \\1)',\n",
    "        r'QUARTER\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'EXTRACT(QUARTER FROM \\1)',\n",
    "        # Text functions\n",
    "        r'CONCATENATE\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'CONCAT(\\1, \\2)',\n",
    "        r'LEFT\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'SUBSTRING(\\1, 1, \\2)',\n",
    "        r'RIGHT\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'SUBSTRING(\\1, -\\2)',\n",
    "        r'LEN\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'LENGTH(\\1)',\n",
    "        # Filter context functions\n",
    "        r'CALCULATE\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'\\1 FILTER BY \\2',\n",
    "        r'FILTER\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'\\1 WHERE \\2',\n",
    "        # Time intelligence functions\n",
    "        r'SAMEPERIODLASTYEAR\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'DATE_SUB(\\1, INTERVAL 1 YEAR)',\n",
    "        r'DATEADD\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'DATE_ADD(\\3, INTERVAL \\2 \\1)',\n",
    "        r'DATEDIFF\\\\s*\\\\(\\\\s*([^,]+),\\\\s*([^,]+),\\\\s*([^)]+)\\\\s*\\\\)': r'DATEDIFF(\\1, \\2, \\3)',\n",
    "        # LOOKUPVALUE/RELATED\n",
    "        r'LOOKUPVALUE\\\\s*\\\\(([^)]+)\\\\)': r'/* LOOKUPVALUE: \\1 */',\n",
    "        r'RELATED\\\\s*\\\\(\\\\s*([^)]+)\\\\s*\\\\)': r'\\1',\n",
    "    }\n",
    "    result = dax_expression\n",
    "    for pattern, replacement in translations.items():\n",
    "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
    "    # Replace table[column] references with table.column\n",
    "    result = re.sub(r'(\\w+)\\[(\\w+)\\]', r'\\1.\\2', result)\n",
    "    # Handle VAR ... RETURN blocks (simple)\n",
    "    if 'VAR' in result:\n",
    "        result = re.sub(r'VAR\\s+(\\w+)\\s*=\\s*([^R]+)RETURN\\s+([^$]+)', r'/* Using: \\1 = \\2 */ \\3', result)\n",
    "    # If fallback is needed, wrap in PySpark UDF\n",
    "    if fallback_to_pyspark:\n",
    "        return f\"# PySpark fallback required\\n# {dax_expression}\\n# Please implement as needed.\"\n",
    "    return result\n",
    "\n",
    "def is_sql_compatible(dax_expression):\n",
    "    # Heuristic: if DAX contains functions not in translations, fallback to PySpark\n",
    "    unsupported = ['GENERATE', 'SUMMARIZE', 'ADDCOLUMNS', 'SELECTCOLUMNS', 'UNION', 'INTERSECT', 'EXCEPT', 'EARLIER', 'PATH', 'PATHITEM']\n",
    "    return not any(fn in dax_expression.upper() for fn in unsupported)\n",
    "\n",
    "df_measures = pd.DataFrame(model['measures'])[['MEASURE_NAME', 'EXPRESSION', 'MEASUREGROUP_NAME']]\n",
    "df_columns = pd.DataFrame(model['columns'])[['TABLE_NAME', 'COLUMN_NAME']]\n",
    "df_calc_cols = pd.DataFrame(model['calculated_columns']) if 'calculated_columns' in model else pd.DataFrame()\n",
    "\n",
    "nb_out = new_notebook()\n",
    "cells_out = []\n",
    "cells_out.append(new_code_cell(\"\"\"# Auto-generated Semantic Layer for Databricks\\nimport pyspark.sql.functions as F\\n\"\"\"))\n",
    "\n",
    "for table in df_columns['TABLE_NAME'].unique():\n",
    "    cols = df_columns[df_columns['TABLE_NAME'] == table]['COLUMN_NAME'].tolist()\n",
    "    meas = df_measures[df_measures['MEASUREGROUP_NAME'] == table]\n",
    "    calc_cols = df_calc_cols[df_calc_cols['TABLE'] == table] if not df_calc_cols.empty else pd.DataFrame()\n",
    "    code = f\"-- Semantic View for: {table}\\n-- Columns: {', '.join(cols)}\\n\"\n",
    "    if materialize:\n",
    "        code += f\"CREATE OR REPLACE VIEW {catalog_name}.{schema_name}.{table}_semantic AS\\nSELECT\\n\"\n",
    "        code += \",\\n\".join([f\"    {c}\" for c in cols])\n",
    "        # Calculated columns\n",
    "        for _, row in calc_cols.iterrows():\n",
    "            expr = row['EXPRESSION']\n",
    "            if is_sql_compatible(expr):\n",
    "                translated = translate_dax_advanced(expr)\n",
    "                code += f\",\\n    {translated} AS {row['NAME']}\"\n",
    "            else:\n",
    "                code += f\",\\n    /* PySpark required for {row['NAME']} */\"\n",
    "        # Measures\n",
    "        for _, row in meas.iterrows():\n",
    "            expr = row['EXPRESSION']\n",
    "            if is_sql_compatible(expr):\n",
    "                translated = translate_dax_advanced(expr)\n",
    "                code += f\",\\n    {translated} AS {row['MEASURE_NAME']}\"\n",
    "            else:\n",
    "                code += f\",\\n    /* PySpark required for {row['MEASURE_NAME']} */\"\n",
    "        code += f\"\\nFROM {catalog_name}.raw.{table};\"\n",
    "    else:\n",
    "        for _, row in calc_cols.iterrows():\n",
    "            expr = row['EXPRESSION']\n",
    "            if is_sql_compatible(expr):\n",
    "                translated = translate_dax_advanced(expr)\n",
    "                code += f\"# {row['NAME']} = {translated}\\n\"\n",
    "            else:\n",
    "                code += f\"# {row['NAME']} requires PySpark implementation\\n\"\n",
    "        for _, row in meas.iterrows():\n",
    "            expr = row['EXPRESSION']\n",
    "            if is_sql_compatible(expr):\n",
    "                translated = translate_dax_advanced(expr)\n",
    "                code += f\"# {row['MEASURE_NAME']} = {translated}\\n\"\n",
    "            else:\n",
    "                code += f\"# {row['MEASURE_NAME']} requires PySpark implementation\\n\"\n",
    "    cells_out.append(new_code_cell(code))\n",
    "\n",
    "nb_out['cells'] = cells_out\n",
    "with open(output_notebook, \"w\") as f:\n",
    "    write(nb_out, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
