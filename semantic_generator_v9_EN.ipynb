{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6540a9",
   "metadata": {},
   "source": [
    "# Power BI Semantic Layer Generator (v9)\n",
    "\n",
    "This notebook generates another notebook that builds a full semantic layer in Databricks from your Power BI dataset.\n",
    "Includes views, DAX measure translations (SQL or PySpark), joins, documentation, and optional Delta materialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c23387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura tus credenciales Power BI y opciones de generaci√≥n\n",
    "client_id = \"TU_CLIENT_ID\"\n",
    "client_secret = \"TU_CLIENT_SECRET\"\n",
    "tenant_id = \"TU_TENANT_ID\"\n",
    "workspace_id = \"TU_WORKSPACE_ID\"\n",
    "dataset_id = \"TU_DATASET_ID\"\n",
    "semantic_model_name = \"Ventas\"\n",
    "materialize = True\n",
    "\n",
    "# Instala dependencias si est√°s en Databricks\n",
    "%pip install msal requests nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ef371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import msal, requests\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "app = msal.ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "token = app.acquire_token_for_client(scopes=[\"https://analysis.windows.net/powerbi/api/.default\"])\n",
    "access_token = token[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d501af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}/tables\", headers=headers).json().get(\"value\", [])\n",
    "relationships = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}/relationships\", headers=headers).json().get(\"value\", [])\n",
    "measures = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}/measures\", headers=headers).json().get(\"value\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import nbformat\n",
    "from textwrap import dedent\n",
    "\n",
    "def convert_dax_to_pyspark(name, expr):\n",
    "    expr = expr.replace(\" \", \"\").upper()\n",
    "    if \"CALCULATE(\" in expr and \"FILTER(\" in expr and \"SUM(\" in expr:\n",
    "        col = expr.split(\"SUM(\")[1].split(\")\")[0].split(\"[\")[-1].split(\"]\")[0]\n",
    "        table = expr.split(\"SUM(\")[1].split(\"[\")[0]\n",
    "        cond_col = expr.split(\"[\")[-2].split(\"]\")[0]\n",
    "        cond_val = expr.split(\">\")[-1].replace(\"))\", \"\")\n",
    "        return dedent(f\"\"\"\n",
    "            from pyspark.sql import functions as F\n",
    "            df = spark.table(\"raw.{table}\")\n",
    "            df_filtered = df.filter(F.col(\"{cond_col}\") > {cond_val})\n",
    "            {name.lower()} = df_filtered.agg(F.sum(\"{col}\")).collect()[0][0]\n",
    "            print(\"{name}:\", {name.lower()})\n",
    "        \"\"\")\n",
    "    elif \"DISTINCTCOUNT(\" in expr:\n",
    "        col = expr.split(\"[\")[-1].split(\"]\")[0]\n",
    "        table = expr.split(\"(\")[1].split(\"[\")[0]\n",
    "        return f'df = spark.table(\"raw.{table}\")\\nresultado = df.select(\"{col}\").distinct().count()'\n",
    "    elif \"AVERAGE(\" in expr:\n",
    "        col = expr.split(\"[\")[-1].split(\"]\")[0]\n",
    "        table = expr.split(\"(\")[1].split(\"[\")[0]\n",
    "        return f'df = spark.table(\"raw.{table}\")\\nresultado = df.agg(F.avg(\"{col}\")).collect()[0][0]'\n",
    "    elif \"DIVIDE(\" in expr:\n",
    "        args = expr.split(\"DIVIDE(\")[1].split(\")\")[0].split(\",\")\n",
    "        num = args[0].split(\"[\")[-1].split(\"]\")[0]\n",
    "        den = args[1].split(\"[\")[-1].split(\"]\")[0]\n",
    "        table = args[0].split(\"[\")[0]\n",
    "        return dedent(f\"\"\"\n",
    "            df = spark.table(\"raw.{table}\")\n",
    "            agg = df.agg(F.sum(\"{num}\").alias(\"num\"), F.sum(\"{den}\").alias(\"den\")).collect()[0]\n",
    "            resultado = agg[\"num\"] / agg[\"den\"] if agg[\"den\"] != 0 else None\n",
    "        \"\"\")\n",
    "    else:\n",
    "        return f\"# ‚ö†Ô∏è No se pudo traducir autom√°ticamente: {expr}\"\n",
    "\n",
    "# Generar nuevo notebook\n",
    "nb2 = new_notebook()\n",
    "nb2.cells.append(new_markdown_cell(f\"# üìä Capa Sem√°ntica Generada: {semantic_model_name}\"))\n",
    "\n",
    "for t in tables:\n",
    "    tname = t[\"name\"]\n",
    "    nb2.cells.append(new_markdown_cell(f\"## Vista base: `{tname}`\"))\n",
    "    nb2.cells.append(new_code_cell(f\"\"\"CREATE OR REPLACE VIEW semantic.{tname} AS\n",
    "SELECT * FROM raw.{tname};\"\"\"))\n",
    "    nb2.cells.append(new_markdown_cell(\"### ‚úçÔ∏è Ejemplo de regla sem√°ntica\"))\n",
    "    nb2.cells.append(new_code_cell(f\"\"\"# from pyspark.sql import functions as F\n",
    "# df = spark.table(\"semantic.{tname}\")\n",
    "# df = df.withColumn(\"BanderaAltaVenta\", F.when(F.col(\"SalesAmount\") > 1000, 1).otherwise(0))\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"semantic.{tname}_ConReglas\")\n",
    "\"\"\"))\n",
    "\n",
    "for r in relationships:\n",
    "    f_table = r[\"fromTable\"]\n",
    "    f_col = r[\"fromColumn\"]\n",
    "    t_table = r[\"toTable\"]\n",
    "    t_col = r[\"toColumn\"]\n",
    "    view = f\"vw_{f_table}\"\n",
    "    nb2.cells.append(new_markdown_cell(f\"## Vista enriquecida: `{view}` con JOIN `{f_col}` = `{t_col}`\"))\n",
    "    nb2.cells.append(new_code_cell(f\"\"\"CREATE OR REPLACE VIEW semantic.{view} AS\n",
    "SELECT f.*, d.*\n",
    "FROM raw.{f_table} f\n",
    "LEFT JOIN raw.{t_table} d ON f.{f_col} = d.{t_col};\"\"\"))\n",
    "    if materialize:\n",
    "        nb2.cells.append(new_code_cell(f\"\"\"CREATE OR REPLACE TABLE semantic.{f_table}_Materializada AS\n",
    "SELECT * FROM semantic.{view};\"\"\"))\n",
    "\n",
    "for m in measures:\n",
    "    name = m[\"name\"]\n",
    "    expr = m[\"expression\"]\n",
    "    nb2.cells.append(new_markdown_cell(f\"### Medida: `{name}`\\n**DAX original:** `{expr}`\"))\n",
    "    if any(fn in expr.upper() for fn in [\"SUM(\", \"COUNT(\", \"AVERAGE(\", \"DISTINCTCOUNT(\", \"CALCULATE(\", \"DIVIDE(\"]):\n",
    "        pyspark = convert_dax_to_pyspark(name, expr)\n",
    "        nb2.cells.append(new_code_cell(pyspark))\n",
    "    else:\n",
    "        nb2.cells.append(new_code_cell(f\"# ‚ö†Ô∏è No se pudo traducir autom√°ticamente: {expr}\"))\n",
    "\n",
    "# Guardar\n",
    "path = f\"/mnt/data/Capa_Semantica_{semantic_model_name}_v9.ipynb\"\n",
    "with open(path, \"w\") as f:\n",
    "    nbformat.write(nb2, f)\n",
    "\n",
    "print(\"‚úÖ Notebook generado:\", path)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
